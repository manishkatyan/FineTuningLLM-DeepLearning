{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3593a7-d398-4701-a583-486661c6ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Tell me how to train my dog to sit. I have a 10 month old puppy and I want to train him to sit. I have tried the treat method and he just sits there and looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks at me like I am crazy. I have tried the \"sit\" command and he just looks\n"
     ]
    }
   ],
   "source": [
    "from llama import BasicModelRunner\n",
    "non_finetuned = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
    "non_finetuned_output = non_finetuned(\"Tell me how to train my dog to sit\")\n",
    "print(non_finetuned_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b54931-3cc3-43bf-b28b-2e002bbdc726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry to hear that. I'll look into it.\n",
      "Customer: I'm not sure if I got the right blanket.\n",
      "Agent: I'm sorry to hear that. I'll look into it.\n",
      "Customer: I'm not sure if I got the right blanket. I'm not sure if I got the right blanket.\n",
      "Agent: I'm sorry to hear that. I'll look into it. I'll look into it.\n",
      "Customer: I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket.\n",
      "Agent: I'm sorry to hear that. I'll look into it. I'll look into it. I'll look into it.\n",
      "Customer: I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blanket. I'm not sure if I got the right blank\n"
     ]
    }
   ],
   "source": [
    "print(non_finetuned(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "Customer: I didn't get my item\n",
    "Agent: I'm sorry to hear that. Which item was it?\n",
    "Customer: the blanket\n",
    "Agent:\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d643f7aa-f5c6-4548-b79a-9296b224d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on command.\n",
      "Training a dog to sit on command is a basic obedience command that can be achieved with patience, consistency, and positive reinforcement. Here's a step-by-step guide on how to train your dog to sit on command:\n",
      "\n",
      "1. Choose a quiet and distraction-free area: Find a quiet area with minimal distractions where your dog can focus on you.\n",
      "2. Have treats ready: Choose your dog's favorite treats and have them ready to use as rewards.\n",
      "3. Stand in front of your dog: Stand in front of your dog and hold a treat close to their nose.\n",
      "4. Move the treat up and back: Slowly move the treat up and back, towards your dog's tail, while saying \"sit\" in a calm and clear voice.\n",
      "5. Dog will sit: As you move the treat, your dog will naturally sit down to follow the treat. The moment their bottom touches the ground, say \"good sit\" and give them the treat.\n",
      "6. Repeat the process: Repeat steps 3-5 several times, so your dog starts to associate the command \"sit\" with\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "finetuned_model_output = finetuned_model(\"Tell me how to train my dog to sit\")\n",
    "print(finetuned_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83736bb0-ed51-4076-a28d-b8810a1d254a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I see. Can you please provide me with your order number so I can look into this for you?\n",
      "Customer: I don't have the order number.\n",
      "Agent: Okay, no worries. Can you please tell me the name of the item you didn't receive?\n",
      "Customer: It was a blue blanket.\n",
      "Agent: I see. I'm going to check on the status of your order for you. Can you please hold for just a moment?\n",
      "Customer: Okay, thank you.\n",
      "Agent: (pause) I apologize, but it looks like your order was delivered to the wrong address. Can you please provide me with your correct address so I can assist you in getting the blanket delivered to you?\n",
      "Customer: (sighs) I don't know why this is happening to me. I've been ordering from Amazon for years and never had any problems.\n",
      "Agent: I apologize for any inconvenience this has caused. I'm here to help you resolve the issue as quickly as possible. Can you please provide me with your correct address so I can assist you?\n",
      "Customer: (frustrated) Fine. It's 123 Main St, Apartment \n"
     ]
    }
   ],
   "source": [
    "print(finetuned_model(\"\"\"Agent: I'm here to help you with your Amazon deliver order.\n",
    "Customer: I didn't get my item\n",
    "Agent: I'm sorry to hear that. Which item was it?\n",
    "Customer: the blanket\n",
    "Agent:\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfbd0d5e-72e9-49f0-8798-d29bababd1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 400\n"
     ]
    },
    {
     "ename": "UserError",
     "evalue": "Currently this user has support for base models: ['hf-internal-testing/tiny-random-gpt2', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', 'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', 'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama-27Oct/lib/python3.9/site-packages/llama/engine/lamini.py:268\u001b[0m, in \u001b[0;36mLamini.make_web_request\u001b[0;34m(self, url, http_method, json)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mHTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama-27Oct/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.powerml.co/v2/lamini/completions",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chatgpt \u001b[38;5;241m=\u001b[39m BasicModelRunner(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mchatgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me how to train my dog to sit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama-27Oct/lib/python3.9/site-packages/llama/runners/basic_model_runner.py:52\u001b[0m, in \u001b[0;36mBasicModelRunner.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Singleton\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     input_objects \u001b[38;5;241m=\u001b[39m Input(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m---> 52\u001b[0m output_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_peft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_peft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output_objects, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     59\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output_objects]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama-27Oct/lib/python3.9/site-packages/llama/engine/typed_lamini.py:13\u001b[0m, in \u001b[0;36mTypedLamini.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 13\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama-27Oct/lib/python3.9/site-packages/llama/engine/lamini.py:86\u001b[0m, in \u001b[0;36mLamini.__call__\u001b[0;34m(self, input, output_type, stop_tokens, model_name, enable_peft, random, max_tokens, streaming)\u001b[0m\n\u001b[1;32m     73\u001b[0m req_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_llm_req_map(\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     75\u001b[0m     model_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     streaming,\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     85\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_web_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/llama-27Oct/lib/python3.9/site-packages/llama/engine/lamini.py:296\u001b[0m, in \u001b[0;36mLamini.make_web_request\u001b[0;34m(self, url, http_method, json)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m         json_response \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UserError(json_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetail\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserError\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m503\u001b[39m:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUserError\u001b[0m: Currently this user has support for base models: ['hf-internal-testing/tiny-random-gpt2', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m-v1', 'EleutherAI/neox-ckpt-pythia-70m-deduped-v1', 'EleutherAI/gpt-neo-125m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-70m', 'EleutherAI/neox-ckpt-pythia-160m', 'EleutherAI/neox-ckpt-pythia-160m-deduped-v1', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/neox-ckpt-pythia-410m', 'EleutherAI/neox-ckpt-pythia-410m-deduped-v1', 'cerebras/Cerebras-GPT-111M', 'cerebras/Cerebras-GPT-256M', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf']"
     ]
    }
   ],
   "source": [
    "#chatgpt = BasicModelRunner(\"chat-gpt\")\n",
    "#print(chatgpt(\"Tell me how to train my dog to sit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65ad32-bf29-4f5d-a9c2-1fe531f59f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
